{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import progressbar\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "from hyperopt import hp\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_Generic import printmd, printHeader, formatLabel\n",
    "from util_Generic import utl_toInteger, utl_isInteger\n",
    "from util_Generic import utl_reduceMemoryUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_FeatureProcess import utl_splitMixedColumnValues\n",
    "from util_FeatureProcess import utl_processColumns\n",
    "from util_FeatureProcess import utl_getValuesCount\n",
    "from util_FeatureProcess import utl_cleanOnDefinition\n",
    "from util_FeatureProcess import utl_cleanDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_PCA import utl_applyPCA\n",
    "from util_PCA import utl_pcaResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustBoostClassifier import CustBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_saveFilenames = { 'featuresDef':             'saveGlobals/featuresDef.json',\n",
    "                    'pcaMissingUnknown':       'saveGlobals/pcaMissingUnknown.sav',\n",
    "                    'dummyEncode':             'saveGlobals/dummyEncode.json',\n",
    "                    'categoricalToProcess':    'saveGlobals/categoricalToProcess.json',\n",
    "                    'categoricalColumns':      'saveGlobals/categoricalColumns.json',\n",
    "                    'highlyCorrelated':        'saveGlobals/highlyCorrelated.json',\n",
    "                    'colMissing':              'saveGlobals/colMissing.json',\n",
    "                    'colUnknownVals':          'saveGlobals/colUnknownVals.json' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "for path in [ '../Arvato Project - dataSensitive', '../../data/Term2/capstone/arvato_data' ]:\n",
    "    try:\n",
    "        azdias    = pd.read_csv(f'{path}/Udacity_AZDIAS_052018.csv', sep=';').drop('Unnamed: 0', axis = 1)            \n",
    "        customers = pd.read_csv(f'{path}/Udacity_CUSTOMERS_052018.csv', sep=';').drop('Unnamed: 0', axis = 1)        \n",
    "        break\n",
    "    except err as Exception: \n",
    "        print(path, err)\n",
    "        \n",
    "utl_reduceMemoryUsage('Azdias', azdias)\n",
    "utl_reduceMemoryUsage('Customers', customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of records in both datasets and display the columns which are missing in one of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of records / features in the general population dataset: {azdias.shape}')\n",
    "print(f'Number of records / features in the customers dataset:          {customers.shape}')\n",
    "print('')\n",
    "print('Columns only available in the general population dataset:', [item for item in azdias.columns if item not in customers.columns])\n",
    "print('Columns only available in the customers dataset:',          [item for item in customers.columns if item not in azdias.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load and process definition for the columns based on the definition file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the definition file there are certain values which are missing, so we will be using the ffill method in order to fill in the gaps.\n",
    "\n",
    "We also create a flag '_isUnknown', which will help we identify the values that have the meaning 'Unknown' in the datasets.\n",
    "\n",
    "We also check all the columns which might be a multi-level categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_featuresDef = pd.read_csv('../Arvato Project - dataSensitive/DIAS Attributes - Values 2017.csv', sep = ';', engine = 'python')\n",
    "g_featuresDef.columns = ['Attribute', 'Description', 'Value', 'Meaning']\n",
    "\n",
    "g_featuresDef['Value Converted'] = g_featuresDef['Value'].apply(lambda x: str(x).split(',')[0].strip()).apply(utl_toInteger)\n",
    "g_featuresDef['Attribute']   = g_featuresDef['Attribute'].ffill()\n",
    "g_featuresDef['Description'] = g_featuresDef['Description'].ffill()\n",
    "\n",
    "for item in ['LP_STATUS_GROB', 'LP_FAMILIE_GROB']:\n",
    "    v_idx = g_featuresDef[g_featuresDef['Attribute'] == item].index\n",
    "    g_featuresDef.loc[v_idx, 'Meaning'] = g_featuresDef.loc[v_idx, 'Meaning'].ffill()\n",
    "\n",
    "# Create the new dictionary containing the definition for the column\n",
    "v_featuresDef = g_featuresDef.T.to_dict()   \n",
    "g_featuresDef = {}\n",
    "for value in v_featuresDef.values():\n",
    "    v_key = value['Attribute']\n",
    "    if not v_key in g_featuresDef.keys():\n",
    "        g_featuresDef[v_key] = { 'Description': value['Description'].strip(),\n",
    "                                 'Values':      {} }    \n",
    "        \n",
    "    if 'unknown' in value['Meaning']:\n",
    "        g_featuresDef[v_key]['Value Unknown'] = value['Value Converted']  \n",
    "    \n",
    "    for item in value['Value'].split(','):  \n",
    "        item = utl_toInteger(item.strip())\n",
    "        if not item in g_featuresDef[v_key]['Values'].keys():\n",
    "            g_featuresDef[v_key]['Values'][item] = {}\n",
    "\n",
    "        g_featuresDef[v_key]['Values'][item]['Meaning'] = value['Meaning']\n",
    "        g_featuresDef[v_key]['Values'][item]['Value Converted'] = value['Value Converted']\n",
    "\n",
    "# Force the creation of a value UNKNOWN if none was provided in the definition file\n",
    "for key in g_featuresDef.keys():\n",
    "    if not 'Value Unknown' in g_featuresDef[key].keys():\n",
    "        g_featuresDef[key]['Value Unknown'] = None\n",
    "        \n",
    "# For the mixed columns in the dataset, calculate the splitted features to be used\n",
    "v_columns = [ 'ALTER_HH', 'CAMEO_DEUINTL_2015', 'PRAEGENDE_JUGENDJAHRE',\n",
    "              'D19_BANKEN_DATUM',  'D19_BANKEN_ONLINE_DATUM',  'D19_BANKEN_OFFLINE_DATUM', \n",
    "              'D19_GESAMT_DATUM',  'D19_GESAMT_ONLINE_DATUM',  'D19_GESAMT_OFFLINE_DATUM',\n",
    "              'D19_TELKO_DATUM',   'D19_TELKO_ONLINE_DATUM',   'D19_TELKO_OFFLINE_DATUM',\n",
    "              'D19_VERSAND_DATUM', 'D19_VERSAND_ONLINE_DATUM', 'D19_VERSAND_OFFLINE_DATUM',\n",
    "              'LP_LEBENSPHASE_GROB', 'LP_LEBENSPHASE_FEIN', \n",
    "              'LP_STATUS_GROB',      'LP_STATUS_FEIN', \n",
    "              'LP_FAMILIE_GROB',     'LP_FAMILIE_FEIN' ]\n",
    "for column in v_columns:\n",
    "    v_values = g_featuresDef[column]['Values']\n",
    "    v_valuesConverted = {}\n",
    "    for key, value in v_values.items():\n",
    "        v_valuesConverted[value['Value Converted']] = { '00_Meaning': value['Meaning'] }\n",
    "    g_featuresDef[column]['Split'] = utl_splitMixedColumnValues( p_column       = column, \n",
    "                                                                 p_values       = v_valuesConverted, \n",
    "                                                                 p_valueUnknown = g_featuresDef[column]['Value Unknown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the split for multi-level categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_data = pd.DataFrame()\n",
    "for columnKey in g_featuresDef.keys():# If columns exists in the definition for a split operation, do the split\n",
    "    if 'Split' in g_featuresDef[columnKey].keys():\n",
    "        v_columns = {key : value['01_Columns'] for key, value in g_featuresDef[columnKey]['Split'].items()}\n",
    "        v_columns = pd.DataFrame(v_columns).T\n",
    "        v_columns.columns = [f'{columnKey}{item}' for item in v_columns.columns]\n",
    "        v_columns.reset_index(inplace = True) \n",
    "        v_columns = v_columns.rename(columns = {'index': f'{columnKey}_MapValue'})             \n",
    "        v_data = pd.concat([v_data, v_columns.head(10)], axis = 1)\n",
    "display(v_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Analysis of categorical features\n",
    "\n",
    "We consider that all values that have the same meaning, should have only one possible value. We will check the definition dictionary and convert all multiple values to only keep the first one.\n",
    "\n",
    "For mixed columns we will use the definition calculated before in order to create the new splitted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_azdias    = utl_cleanOnDefinition( p_label       = 'Azdias',\n",
    "                                     p_data        = azdias, \n",
    "                                     p_featuresDef = g_featuresDef,\n",
    "                                     p_showHeader  = True )\n",
    "\n",
    "v_customers = utl_cleanOnDefinition( p_label       = 'Customers',\n",
    "                                     p_data        = customers, \n",
    "                                     p_featuresDef = g_featuresDef,\n",
    "                                     p_showHeader  = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Column 'EINGEFUEGT_AM' contains an encoding of type timestamp, so we will convert it to a timestamp and extract its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(v_azdias['EINGEFUEGT_AM'].value_counts()).head())\n",
    "utl_processColumns(v_azdias,    'EINGEFUEGT_AM')\n",
    "utl_processColumns(v_customers, 'EINGEFUEGT_AM')\n",
    "display(v_azdias.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Try to convert all columns having type object, both which can be encoded as float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in v_azdias.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        v_azdias[column]    = v_azdias[column].astype(np.float16)\n",
    "        v_customers[column] = v_customers[column].astype(np.float16)\n",
    "    except: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Identify categorical like columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_categoryColumns    = v_azdias.drop(['LNR'], axis = 1).describe().T.reset_index()\n",
    "v_categoryColumns    = v_categoryColumns[(v_categoryColumns['min'] >= -2) & (v_categoryColumns['max'] <= 10)].set_index('index')\n",
    "g_categoricalColumns = v_categoryColumns.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Check columns having type 'object' and do a processing using a custom mapping for the values. Show all the columns for which a mapping **is not** defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_objectCols = v_azdias.select_dtypes(include=['object']).copy()  \n",
    "g_categoricalToProcess = []\n",
    "for column in v_objectCols.columns:    \n",
    "    try:\n",
    "        utl_processColumns(v_objectCols, column)\n",
    "        g_categoricalToProcess.append(column)\n",
    "    except Exception as err: \n",
    "        print('\\n*******************************************************')\n",
    "        print(f'Columns {column} could not be mapped:')    \n",
    "        print(err)\n",
    "        print(sorted(v_objectCols[column].astype(str).value_counts().index.tolist()))\n",
    "        \n",
    "g_categoricalToProcess.append('PRAEGENDE_JUGENDJAHRE_Value_0')\n",
    "g_categoricalToProcess = sorted(set(g_categoricalToProcess))\n",
    "\n",
    "g_categoricalColumns.extend(g_categoricalToProcess)\n",
    "g_categoricalColumns.append('ALTERSKATEGORIE_FEIN')\n",
    "g_categoricalColumns = sorted(set(g_categoricalColumns))\n",
    "\n",
    "print('\\n*******************************************************')\n",
    "print('Columns for which a mapping is defined:')   \n",
    "display(g_categoricalToProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Analysis of MISSING / UNKNOWN values\n",
    "\n",
    "Calculate all the possible values and their distribution in both dataframes.\n",
    "\n",
    "Missing values could follow a specific pattern, so we take into account that we can have the following situations:\n",
    "   - data is **Missing Completely at Random (MCAR)** - no relationship between the missingness of the data and any values, observed or missing (nothing systematic going on)\n",
    "   - data is **Missing at Random (MAR)** - we have a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual’s observed variables (eg: women are less-likely to tell their age or weight)\n",
    "   - data is **Missing Not at Random (MNAR)** - there is a relationship between the propensity of a value to be missing and its values. This is a case where the people with the lowest education are missing on education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_valuesCount = utl_getValuesCount( p_azdias      = v_azdias, \n",
    "                                    p_customers   = v_customers,\n",
    "                                    p_featuresDef = g_featuresDef )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "We start by checking for a pattern for **MISSING** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_null = ( g_valuesCount[g_valuesCount['Value'].isnull()]\n",
    "                 .groupby(['Value %_Azdias', 'Value %_Cust'])\n",
    "                 .agg({'Column Name': ['count']}) )\n",
    "v_null.columns = ['Count']\n",
    "display(v_null[v_null['Count'] != 1].sort_values('Count', ascending = False).head(10).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as there are multiple cases where we can see a clear correlation between the number of missing values for the different features, so we can consider that the values are **Missing Not at Random (MNAR)**.\n",
    "\n",
    "------\n",
    "We also check for a pattern for **UNKNOWN** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_unknown = ( g_valuesCount[g_valuesCount['_isUnknown'] == 1]\n",
    "                    .groupby(['Value %_Azdias', 'Value %_Cust'])\n",
    "                    .agg({'Column Name': ['count']}) )\n",
    "v_unknown.columns = ['Count']\n",
    "display(v_unknown[v_unknown['Count'] != 1].sort_values('Count', ascending = False).head(10).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as there are multiple cases where we can see a clear correlation between the number of unknown values for the different features, so we can consider that the values are **Missing Not at Random (MNAR)**.\n",
    "\n",
    "-----\n",
    "As we know that we are in a MNAR situation, we will execute a **Principal component analysis (PCA)** on both **MISSING** and **UNKNOWN** data in order to identify the missing / unknown flags with a reduced dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_missingAzdias = v_azdias[['LNR']].copy()\n",
    "v_missingCust   = v_customers[['LNR']].copy()\n",
    "\n",
    "v_cols = v_azdias.columns.tolist()\n",
    "with progressbar.ProgressBar(max_value = len(v_cols)) as bar:\n",
    "    v_count = 0\n",
    "    for column in v_cols:\n",
    "        v_missingAzdias[column] = v_azdias[column].isnull().astype(int)\n",
    "        v_missingCust[column]   = v_customers[column].isnull().astype(int)\n",
    "\n",
    "        # In case we have only one value in both dataframes (which means that the column is always filled in) than we remove the\n",
    "        # column as it will not bring any information.\n",
    "        if ( v_missingAzdias[column].value_counts().shape[0] == 1\n",
    "             and v_missingCust[column].value_counts().shape[0] == 1 ):\n",
    "            v_missingAzdias.drop(column, axis = 1, inplace = True)\n",
    "            v_missingCust.drop(column, axis = 1, inplace = True)\n",
    "            \n",
    "        v_count += 1\n",
    "        bar.update(v_count)\n",
    "v_missingAzdias.columns = [f'_isMissing_{item}' for item in v_missingAzdias.columns]\n",
    "        \n",
    "# Save the list of columns for which we will flag if values are missing or not        \n",
    "g_colMissing = v_missingAzdias.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_unknownAzdias = v_azdias[['AGER_TYP']].copy()\n",
    "v_unknownCust   = v_customers[['AGER_TYP']].copy()\n",
    "\n",
    "g_colUnknownVals = {}\n",
    "for column in g_valuesCount[g_valuesCount['_isUnknown'] == 1]['Column Name'].tolist():\n",
    "    g_colUnknownVals[column] = g_valuesCount[   (g_valuesCount['_isUnknown'] == 1) \n",
    "                                              & (g_valuesCount['Column Name'] == column) ]['Value'].values[0]\n",
    "\n",
    "v_cols = g_colUnknownVals.keys()\n",
    "with progressbar.ProgressBar(max_value = len(v_cols)) as bar:\n",
    "    v_count = 0\n",
    "    for column in v_cols:\n",
    "        v_value = g_colUnknownVals[column]\n",
    "        v_unknownAzdias[column] = v_azdias[column].fillna(-999).apply(lambda x: 1 if x == v_value else 0 )\n",
    "        v_unknownCust[column]   = v_customers[column].fillna(-999).apply(lambda x: 1 if x == v_value else 0 )\n",
    "\n",
    "        # In case we have only one value in both dataframes (which means that the column is always known) than we remove the\n",
    "        # column as it will not bring any information.\n",
    "        if ( v_unknownAzdias[column].value_counts().shape[0] == 1\n",
    "             and v_unknownCust[column].value_counts().shape[0] == 1 ):\n",
    "            v_unknownAzdias.drop(column, axis = 1, inplace = True)\n",
    "            v_unknownCust.drop(column, axis = 1, inplace = True)\n",
    "            \n",
    "        v_count += 1\n",
    "        bar.update(v_count)       \n",
    "v_unknownAzdias.columns = [f'_isUnknown_{item}' for item in v_unknownAzdias.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calculate the PCA for **MISSING** and **UNKNOWN** values and show the report for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_missingUnknownAzdias = pd.concat([v_missingAzdias, v_unknownAzdias], axis = 1)\n",
    "g_pcaMissingUnknown = utl_applyPCA( p_data         = v_missingUnknownAzdias,\n",
    "                                    p_n_components = 60,\n",
    "                                    p_ShowWeights  = True,\n",
    "                                    p_ShowTop      = 6,\n",
    "                                    p_figHeight    = 24 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based of the visualisation above, we will select **20 dimensions** for the PCA, as they capture almost 100% of the total variance for unknown and missing values. \n",
    "We do a retrain for the PCA with the selected number of components, and save the trained PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pcaMissingUnknown = utl_applyPCA( p_data         = v_missingUnknownAzdias,\n",
    "                                    p_n_components = 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Fill-in missing values\n",
    "\n",
    "We first re-encode some categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in g_categoricalToProcess:\n",
    "    utl_processColumns(v_azdias, item)\n",
    "    utl_processColumns(v_customers, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Dummy encode the object columns not re-encoded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dummyEncode = {}\n",
    "for column in v_azdias.select_dtypes(include=['object']).columns:\n",
    "    v_azdias[column] = v_azdias[column].str.replace('/', '-').str.replace(' ', '_')\n",
    "    v_dummy = pd.get_dummies(v_azdias[[column]])\n",
    "    g_dummyEncode[column] = v_dummy.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceDummy(p_data, p_column, p_dummyEncode):\n",
    "    if p_column not in p_data.columns: \n",
    "        for column in p_dummyEncode[p_column]:\n",
    "            p_data[column] = 0\n",
    "        return p_data\n",
    "    \n",
    "    p_data[p_column] = p_data[p_column].str.replace('/', '-').str.replace(' ', '_')\n",
    "    p_data = pd.get_dummies(p_data, columns = [p_column])\n",
    "    \n",
    "    v_cols = [item for item in p_data.columns if (p_column in item and item not in p_dummyEncode[p_column])]\n",
    "    if len(v_cols) > 0:\n",
    "        p_data.drop(v_cols, axis = 1, inplace = True)\n",
    "    \n",
    "    for column in [item for item in p_dummyEncode[p_column] if item not in p_data.columns]:\n",
    "        p_data[column] = 0\n",
    "    \n",
    "    return p_data\n",
    "     \n",
    "for column in list(g_dummyEncode.keys()):\n",
    "    v_azdias    = replaceDummy( p_data        = v_azdias, \n",
    "                                p_column      = column, \n",
    "                                p_dummyEncode = g_dummyEncode )\n",
    "    v_customers = replaceDummy( p_data        = v_customers, \n",
    "                                p_column      = column, \n",
    "                                p_dummyEncode = g_dummyEncode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fill in columns that are categorical with their unknown value or -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with progressbar.ProgressBar(max_value = len(g_categoricalColumns)) as bar:\n",
    "    v_count = 0\n",
    "    for column in g_categoricalColumns:\n",
    "        v_azdias[column]    = v_azdias[column].fillna(-2).astype(int)\n",
    "        v_customers[column] = v_customers[column].fillna(-2).astype(int)\n",
    "        v_count += 1\n",
    "        bar.update(v_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check the columns that still have empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dummyEncode = []\n",
    "for value in g_dummyEncode.values():\n",
    "    v_dummyEncode.extend(value)\n",
    "display(sorted([item for item in v_azdias.columns if (item not in g_categoricalColumns and item not in v_dummyEncode)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_missingAzdias = pd.DataFrame({'Null Value': 0}, index = ['LNR'])\n",
    "v_missingCust   = pd.DataFrame({'Null Value': 0}, index = ['LNR'])\n",
    "for column in v_azdias.columns:\n",
    "    v_missingAzdias.loc[column, 'Null Value'] = v_azdias[column].isnull().sum()\n",
    "    v_missingCust.loc[column, 'Null Value']   = v_customers[column].isnull().sum()\n",
    "v_missing = ( v_missingAzdias[v_missingAzdias['Null Value'] > 0]\n",
    "                 .merge( v_missingCust[v_missingCust['Null Value'] > 0], \n",
    "                         how = 'outer', left_index = True, right_index = True,\n",
    "                         suffixes = ('_Azdias', '_Cust') ) )\n",
    "display(v_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### Check for highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "v_corr_matrix = v_azdias.drop('LNR', axis = 1).corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "v_upper = v_corr_matrix.where(np.triu(np.ones(v_corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "v_to_drop = [column for column in v_upper.columns if any(v_upper[column] > 0.95)]\n",
    "\n",
    "for column in v_to_drop:\n",
    "    v_value = v_corr_matrix.drop(column, axis = 1).loc[column, :]\n",
    "    v_value = v_value[v_value > 0.97]\n",
    "    if v_value.shape[0] > 0: display(pd.DataFrame(v_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_highlyCorrelated = [ \"DSL_FLAG\", \"ALTER_HH_Value_1\", \"EINGEFUEGT_AM_Year\", \"EINGEFUEGT_AM_Month\", \"EINGEFUEGT_AM_Quarter\", \n",
    "                       \"EINGEFUEGT_AM_Hour\", \"EINGEFUEGT_AM_Minute\", \"EINGEFUEGT_AM_Second\", \"EINGEFUEGT_AM_WeekOfYear\", \n",
    "                       \"LP_LEBENSPHASE_GROB_Value_FAMILY\", \"LP_LEBENSPHASE_FEIN_Value_FAMILY\", \"ANZ_TITEL\", \n",
    "                       \"ANZ_STATISTISCHE_HAUSHALTE\", \"PLZ8_HHZ\", \"PLZ8_GBZ\", \"KBA13_SEG_GROSSRAUMVANS\", \"KBA13_KMH_211\", \n",
    "                       \"KBA13_ALTERHALTER_61\", \"KBA13_ALTERHALTER_60\", \"KBA13_ALTERHALTER_45\", \"KBA13_ALTERHALTER_30\", \n",
    "                       \"KBA05_KRSKLEIN\", \"KBA05_HERST3\", \"KBA05_HERST2\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Save all global variables / models for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(g_pcaMissingUnknown, g_saveFilenames['pcaMissingUnknown'])\n",
    "\n",
    "with open(g_saveFilenames['featuresDef'], 'w') as outfile:\n",
    "    json.dump(g_featuresDef, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalColumns'], 'w') as outfile:\n",
    "    json.dump(g_categoricalColumns, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalToProcess'], 'w') as outfile:\n",
    "    json.dump(g_categoricalToProcess, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['dummyEncode'], 'w') as outfile:\n",
    "    json.dump(g_dummyEncode, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['highlyCorrelated'], 'w') as outfile:\n",
    "    json.dump(g_highlyCorrelated, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['colMissing'], 'w') as outfile:\n",
    "    json.dump(g_colMissing, outfile)\n",
    "    \n",
    "with open(g_saveFilenames['colUnknownVals'], 'w') as outfile:\n",
    "    json.dump(g_colUnknownVals, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-------------\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Clean both datasets\n",
    "\n",
    "Based on finding above, we will create a cleaning procedure that we will apply on all future datasets having the same format.\n",
    "\n",
    "For the cleaning of datasets, we will execute the following operations:\n",
    "   1. **clean data based on the definition** - only one possible value will be kept when there were multiple values specififed for the same row in the definitions\n",
    "   2. re-encode categorical features\n",
    "   3. apply the Principal Component Analysis for the **missing values**, by using the trained PCA from above\n",
    "   4. apply the Principal Component Analysis for the **unknow values**, by using the trained PCA from above\n",
    "   5. fill in categorical columns with -2\n",
    "   6. drop features identified as highly correlated\n",
    "   7. calculate columns to be imputed later\n",
    "   8. calculate columns to be scaled later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load variables saved earlier\n",
    "v_pcaMissingUnknown = joblib.load(g_saveFilenames['pcaMissingUnknown'])\n",
    "\n",
    "with open(g_saveFilenames['featuresDef'], 'r') as inFile:\n",
    "    v_featuresDef = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalColumns'], 'r') as inFile:\n",
    "    v_categoricalColumns = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalToProcess'], 'r') as inFile:\n",
    "    v_categoricalToProcess = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['dummyEncode'], 'r') as inFile:\n",
    "    v_dummyEncode = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['highlyCorrelated'], 'r') as inFile:\n",
    "    v_highlyCorrelated = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colMissing'], 'r') as inFile:\n",
    "    v_colMissing = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colUnknownVals'], 'r') as inFile:\n",
    "    v_colUnknownVals = json.load(inFile)\n",
    "    \n",
    "v_azdias, v_missing, v_scaleColumns = utl_cleanDataFrame( p_label                 = 'Azdias', \n",
    "                                                          p_data                  = azdias, \n",
    "                                                          p_featuresDef           = v_featuresDef, \n",
    "                                                          p_categoricalColumns    = v_categoricalColumns, \n",
    "                                                          p_categoricalToProcess  = v_categoricalToProcess, \n",
    "                                                          p_dummyEncode           = v_dummyEncode, \n",
    "                                                          p_highlyCorrelated      = v_highlyCorrelated, \n",
    "                                                          p_pcaMissingUnknown     = v_pcaMissingUnknown, \n",
    "                                                          p_colMissing            = v_colMissing, \n",
    "                                                          p_colUnknownVals        = v_colUnknownVals,\n",
    "                                                          p_ignoreScale           = [ 'LNR' ] )\n",
    "    \n",
    "v_customers, _, _ = utl_cleanDataFrame( p_label                 = 'Customers', \n",
    "                                        p_data                  = customers, \n",
    "                                        p_featuresDef           = v_featuresDef, \n",
    "                                        p_categoricalColumns    = v_categoricalColumns, \n",
    "                                        p_categoricalToProcess  = v_categoricalToProcess, \n",
    "                                        p_dummyEncode           = v_dummyEncode, \n",
    "                                        p_highlyCorrelated      = v_highlyCorrelated, \n",
    "                                        p_pcaMissingUnknown     = v_pcaMissingUnknown, \n",
    "                                        p_colMissing            = v_colMissing, \n",
    "                                        p_colUnknownVals        = v_colUnknownVals,\n",
    "                                        p_ignoreScale           = [ 'LNR' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of records in both datasets and display the columns which are missing in one of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of records / features in the general population dataset: {v_azdias.shape}')\n",
    "print(f'Number of records / features in the customers dataset:          {v_customers.shape}')\n",
    "print('')\n",
    "print('Columns only available in the general population dataset:', [item for item in v_azdias.columns if item not in v_customers.columns])\n",
    "print('Columns only available in the customers dataset:',          [item for item in v_customers.columns if item not in v_azdias.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\___winApp\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "START Clean dataframe **<span style=\"color: blue\">Train Dataset</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Clean dataframe **<span style=\"color: blue\">Train Dataset</span>** based on definition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode columns values based on definition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (314 of 314) |######################| Elapsed Time: 0:00:46 Time:  0:00:46\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Transform object columns to numeric."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Reduce memory usage for dataframe **<span style=\"color: blue\">Train Dataset</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (4 of 392) |                        | Elapsed Time: 0:00:00 ETA:   0:00:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage is 106.94 MBs.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42962 entries, 0 to 42961\n",
      "Columns: 392 entries, LNR to PRAEGENDE_JUGENDJAHRE_Value_3\n",
      "dtypes: float16(89), float64(193), int64(87), object(23)\n",
      "memory usage: 106.9+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (392 of 392) |######################| Elapsed Time: 0:00:05 Time:  0:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final memory usage is 33.92 MBs.\n",
      "This is 31.72% of the initial size.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42962 entries, 0 to 42961\n",
      "Columns: 392 entries, LNR to PRAEGENDE_JUGENDJAHRE_Value_3\n",
      "dtypes: float16(263), int8(6), object(23), uint16(1), uint32(1), uint8(98)\n",
      "memory usage: 33.9+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode column **<span style=\"color: blue\">EINGEFUEGT_AM</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode column having year format **<span style=\"color: blue\">GEBURTSJAHR</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode column having year format **<span style=\"color: blue\">MIN_GEBAEUDEJAHR</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode column having year format **<span style=\"color: blue\">EINGEZOGENAM_HH_JAHR</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode column having year format **<span style=\"color: blue\">EINGEFUEGT_AM_Year</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Calculate the reduced dimension flags for **<span style=\"color: blue\">MISSING</span>** and **<span style=\"color: blue\">UNKNOWN</span>** values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Concatenate the calculated missing / unknonw with reduced dimension."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Re-encode some categorical columns."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Dummy encode the object columns not re-encoded above."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Fill in categorical columns with -2."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Drop features identified as highly correlated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Calculate columns to be imputed later."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Calculate columns to be scaled later."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "END Clean dataframe **<span style=\"color: blue\">Train Dataset</span>**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for path in [ '../Arvato Project - dataSensitive', '../../data/Term2/capstone/arvato_data' ]:\n",
    "    try:\n",
    "        mailout_train = pd.read_csv(f'{path}/Udacity_MAILOUT_052018_TRAIN.csv', sep=';').drop('Unnamed: 0', axis = 1)\n",
    "        break\n",
    "    except: None\n",
    "        \n",
    "# Load variables saved earlier\n",
    "v_pcaMissingUnknown = joblib.load(g_saveFilenames['pcaMissingUnknown'])\n",
    "\n",
    "with open(g_saveFilenames['featuresDef'], 'r') as inFile:\n",
    "    v_featuresDef = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalColumns'], 'r') as inFile:\n",
    "    v_categoricalColumns = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalToProcess'], 'r') as inFile:\n",
    "    v_categoricalToProcess = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['dummyEncode'], 'r') as inFile:\n",
    "    v_dummyEncode = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['highlyCorrelated'], 'r') as inFile:\n",
    "    v_highlyCorrelated = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colMissing'], 'r') as inFile:\n",
    "    v_colMissing = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colUnknownVals'], 'r') as inFile:\n",
    "    v_colUnknownVals = json.load(inFile)\n",
    "    \n",
    "v_mailout_train, v_missing, v_scaleColumns = utl_cleanDataFrame( p_label                 = 'Train Dataset', \n",
    "                                                                 p_data                  = mailout_train, \n",
    "                                                                 p_featuresDef           = v_featuresDef, \n",
    "                                                                 p_categoricalColumns    = v_categoricalColumns, \n",
    "                                                                 p_categoricalToProcess  = v_categoricalToProcess, \n",
    "                                                                 p_dummyEncode           = v_dummyEncode, \n",
    "                                                                 p_highlyCorrelated      = v_highlyCorrelated, \n",
    "                                                                 p_pcaMissingUnknown     = v_pcaMissingUnknown, \n",
    "                                                                 p_colMissing            = v_colMissing, \n",
    "                                                                 p_colUnknownVals        = v_colUnknownVals,\n",
    "                                                                 p_ignoreScale           = [ 'LNR', 'RESPONSE' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Split in train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "v_features = v_mailout_train.drop(['RESPONSE', 'LNR'], axis = 1)\n",
    "v_target   = v_mailout_train['RESPONSE']\n",
    "X_trainAll, X_testAll, y_trainAll, y_testAll = train_test_split( v_features, \n",
    "                                                                 v_target, \n",
    "                                                                 test_size = .10, \n",
    "                                                                 random_state = 42,\n",
    "                                                                 stratify = v_target )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Define the weight for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive weight for the complete target:  79.756\n",
      "Positive weight for the train dataset:    79.72\n",
      "Positive weight for the test dataset:     80.075\n"
     ]
    }
   ],
   "source": [
    "print('Positive weight for the complete target: ', round(float(np.sum(v_target == 0)) / np.sum(v_target == 1), 3))\n",
    "print('Positive weight for the train dataset:   ', round(float(np.sum(y_trainAll == 0)) / np.sum(y_trainAll == 1), 3))\n",
    "print('Positive weight for the test dataset:    ', round(float(np.sum(y_testAll == 0)) / np.sum(y_testAll == 1), 3))\n",
    "v_posWeight = 79.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian search for parameters for LGB models with all features, no imputing, no scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute CV split.\n",
      "Class has been initialized for model type: lightgbm\n"
     ]
    }
   ],
   "source": [
    "v_hyper_space = { 'objective':           'binary',\n",
    "                  'n_estimators': 1000 + hp.randint('n_estimators', 1500),               \n",
    "                  'boosting_type': hp.choice('boosting_type', [\n",
    "                                                         { 'boosting_type': 'gbdt', \n",
    "                                                           'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                         { 'boosting_type': 'dart', \n",
    "                                                           'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                         { 'boosting_type': 'goss'} ]),\n",
    "                  'max_depth':         hp.quniform('max_depth', 1, 30, 1),\n",
    "                  'scale_pos_weight':  hp.quniform('scale_pos_weight', v_posWeight * 0.5, \n",
    "                                                                       v_posWeight * 2, \n",
    "                                                                       v_posWeight * 0.5),\n",
    "                  'num_leaves':        hp.quniform('num_leaves', 2, 150, 1),\n",
    "                  'learning_rate':     hp.loguniform('learning_rate', np.log(0.00001), np.log(0.2)),\n",
    "                  'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "                  'reg_alpha':         hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "                  'reg_lambda':        hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "                  'colsample_bytree':  hp.uniform('colsample_by_tree', 0.6, 1.0) }  \n",
    "\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_modelName          = 'LGB_AllFeatures_NI_NS',\n",
    "                               X_train              = X_trainAll.copy(),\n",
    "                               y_train              = y_trainAll.copy(),\n",
    "                               p_selected_features  = [],\n",
    "                               p_imputeColumns      = [], \n",
    "                               p_scaleColumns       = [], \n",
    "                               p_cvSplits           = 10,\n",
    "                               p_random_state       = 2013 )   \n",
    "v_bayes_trials = v_model.bayesianSearchLGB( p_hyper_space = v_hyper_space,\n",
    "                                            p_posWeight   = v_posWeight,\n",
    "                                            p_max_eval    = 30,\n",
    "                                            X_test        = X_testAll, \n",
    "                                            y_test        = y_testAll )\n",
    "v_stackDir = v_model.copyStackModels(p_tops = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Make predictions with best 10 stack models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_stackDir = ['']\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_random_state       = 2013 )   \n",
    "y_pred = v_model.predictStack( p_folder   = v_stackDir,\n",
    "                               X_data     = X_testAll, \n",
    "                               y_data     = y_testAll, \n",
    "                               p_showPlot = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Calculate most 30 important features, based on the models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_stackDir = ['']\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_random_state       = 2013 )\n",
    "v_mainFeatures = v_model.featureImportanceStack( p_folder = v_stackDir,\n",
    "                                                 p_top    = 30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian search for parameters for LGB models with 30 features, no imputing, no scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_hyper_space = { 'objective':           'binary',\n",
    "                  'n_estimators': 1000 + hp.randint('n_estimators', 1500),               \n",
    "                  'boosting_type': hp.choice('boosting_type', [\n",
    "                                                         { 'boosting_type': 'gbdt', \n",
    "                                                           'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                         { 'boosting_type': 'dart', \n",
    "                                                           'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                         { 'boosting_type': 'goss'} ]),\n",
    "                  'max_depth':         hp.quniform('max_depth', 1, 30, 1),\n",
    "                  'scale_pos_weight':  hp.quniform('scale_pos_weight', v_posWeight * 0.5, \n",
    "                                                                       v_posWeight * 2.5, \n",
    "                                                                       v_posWeight * 0.1),\n",
    "                  'num_leaves':        hp.quniform('num_leaves', 2, 150, 1),\n",
    "                  'learning_rate':     hp.loguniform('learning_rate', np.log(0.00001), np.log(0.2)),\n",
    "                  'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "                  'reg_alpha':         hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "                  'reg_lambda':        hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "                  'colsample_bytree':  hp.uniform('colsample_by_tree', 0.6, 1.0) } \n",
    "\n",
    "v_selected_features = v_mainFeatures['feature'].tolist()[:30]\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_modelName          = 'LGB_Sel30_Features_NI_NS',\n",
    "                               X_train              = X_trainAll.copy(),\n",
    "                               y_train              = y_trainAll.copy(),\n",
    "                               p_selected_features  = v_selected_features,\n",
    "                               p_imputeColumns      = [], \n",
    "                               p_scaleColumns       = [], \n",
    "                               p_cvSplits           = 10,\n",
    "                               p_random_state       = 2013 )   \n",
    "v_bayes_trials_FS30 = v_model.bayesianSearchLGB( p_hyper_space  = v_hyper_space,\n",
    "                                                 p_posWeight    = v_posWeight,\n",
    "                                                 p_max_eval     = 30,\n",
    "                                                 X_test         = X_testAll, \n",
    "                                                 y_test         = y_testAll )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Visualize the distibution of the scores for the different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian did not find an optimum after 600 trials, so we will check the distribution of the scores for different parameters, and do a new search on narrowed hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['learning_rate'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['learning_rate'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best 150 trials have a learning rate lower than 0.0075."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['num_leaves'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['num_leaves'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the best 150 trials we have an optimal number of leaves lower than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['colsample_bytree'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['colsample_bytree'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the best 150 trials we have an optimal number of column samples by tree between 0.65 and 0.86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['reg_lambda'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['reg_lambda'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the best 150 trials we have an optimal number for reg lambda lower than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['min_child_samples'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['min_child_samples'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()\n",
    "\n",
    "vt_bayes_trials = sorted(v_bayes_trials, key=lambda x: x['scoreValid'], reverse = True)\n",
    "for item in vt_bayes_trials[:150]:\n",
    "    plt.scatter(round(item['params']['scale_pos_weight'], 4), item['scoreValid'], c = \"g\")\n",
    "    plt.scatter(round(item['params']['scale_pos_weight'], 4), item['scoreTest'], c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_hyper_space = { 'objective':           'binary',\n",
    "                  'n_estimators': 1000 + hp.randint('n_estimators', 1500),               \n",
    "                  'boosting_type': hp.choice('boosting_type', [\n",
    "                                                         { 'boosting_type': 'gbdt', \n",
    "                                                           'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                         { 'boosting_type': 'dart', \n",
    "                                                           'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                         { 'boosting_type': 'goss'} ]),\n",
    "                  'max_depth':         hp.quniform('max_depth', 1, 60, 1),\n",
    "                  'scale_pos_weight':  hp.quniform('scale_pos_weight', v_posWeight, \n",
    "                                                                       v_posWeight * 2,\n",
    "                                                                       v_posWeight * 0.2 ),\n",
    "                  'num_leaves':        hp.quniform('num_leaves', 2, 40, 1),\n",
    "                  'learning_rate':     hp.loguniform('learning_rate', np.log(0.000001), np.log(0.0075)),\n",
    "                  'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "                  'reg_alpha':         hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "                  'reg_lambda':        hp.uniform('reg_lambda', 0.0, 0.5),\n",
    "                  'colsample_bytree':  hp.uniform('colsample_by_tree', 0.65, 0.86) }  \n",
    "\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_modelName          = 'LGB_AllFeatures_NI_NS_RP',\n",
    "                               X_train              = X_trainAll.copy(),\n",
    "                               y_train              = y_trainAll.copy(),\n",
    "                               p_selected_features  = [],\n",
    "                               p_imputeColumns      = [], \n",
    "                               p_scaleColumns       = [], \n",
    "                               p_cvSplits           = 10,\n",
    "                               p_random_state       = 2013 )   \n",
    "v_bayes_trials = v_model.bayesianSearchLGB( p_hyper_space = v_hyper_space,\n",
    "                                            p_posWeight   = v_posWeight,\n",
    "                                            p_max_eval    = 900,\n",
    "                                            p_topTrials   = 30,\n",
    "                                            X_test        = X_testAll, \n",
    "                                            y_test        = y_testAll )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian search for parameters for a LGB model with selected features, no imputing, no scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian search for parameters for a LGB model with selected features, imputing and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer – this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [ '../Arvato Project - dataSensitive', '../../data/Term2/capstone/arvato_data' ]:\n",
    "    try:\n",
    "        mailout_test = pd.read_csv(f'{path}/Udacity_MAILOUT_052018_TEST.csv', sep=';').drop('Unnamed: 0', axis = 1)\n",
    "        break\n",
    "    except: None\n",
    "        \n",
    "# Load variables saved earlier\n",
    "v_pcaMissingUnknown = joblib.load(g_saveFilenames['pcaMissingUnknown'])\n",
    "\n",
    "with open(g_saveFilenames['featuresDef'], 'r') as inFile:\n",
    "    v_featuresDef = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalColumns'], 'r') as inFile:\n",
    "    v_categoricalColumns = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['categoricalToProcess'], 'r') as inFile:\n",
    "    v_categoricalToProcess = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['dummyEncode'], 'r') as inFile:\n",
    "    v_dummyEncode = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['highlyCorrelated'], 'r') as inFile:\n",
    "    v_highlyCorrelated = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colMissing'], 'r') as inFile:\n",
    "    v_colMissing = json.load(inFile)\n",
    "    \n",
    "with open(g_saveFilenames['colUnknownVals'], 'r') as inFile:\n",
    "    v_colUnknownVals = json.load(inFile)\n",
    "    \n",
    "v_mailout_test, v_missing, v_scaleColumns = utl_cleanDataFrame( p_label                 = 'Test Dataset', \n",
    "                                                                p_data                  = mailout_test, \n",
    "                                                                p_featuresDef           = v_featuresDef, \n",
    "                                                                p_categoricalColumns    = v_categoricalColumns, \n",
    "                                                                p_categoricalToProcess  = v_categoricalToProcess, \n",
    "                                                                p_dummyEncode           = v_dummyEncode, \n",
    "                                                                p_highlyCorrelated      = v_highlyCorrelated, \n",
    "                                                                p_pcaMissingUnknown     = v_pcaMissingUnknown, \n",
    "                                                                p_colMissing            = v_colMissing, \n",
    "                                                                p_colUnknownVals        = v_colUnknownVals,\n",
    "                                                                p_ignoreScale           = [ 'LNR' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_folder = 'LGB_AllFeatures_NI_NS_RT_20181202_1035'\n",
    "v_model = CustBoostClassifier( p_modelType          = 'lightgbm',\n",
    "                               p_random_state       = 2013 )   \n",
    "y_pred = v_model.predictStack( p_folder   = [ v_folder ],\n",
    "                               X_data     = v_mailout_test.drop(['LNR'], axis = 1) )\n",
    "\n",
    "y_submit = pd.DataFrame(y_pred, columns = ['RESPONSE'])\n",
    "y_submit['LNR'] = v_mailout_test['LNR']\n",
    "y_submit = y_submit[['LNR', 'RESPONSE']]\n",
    "y_submit.to_csv(f'submit/submit_{v_folder}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
